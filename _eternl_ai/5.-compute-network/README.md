---
permalink: /eternl_ai/5.-compute-network/
layout: single
author_profile: false
sidebar:
  nav: "eternl_ai"
---

# 5. Compute Network

Our Compute Network provides reliable and affordable computing resources for AI game inferencing. It optimizes aggregation, distribution, and verification of game compute supply provided by decentralized physical infrastructure networks (DePINs), centralized GPU services, miners, and players equipped with high-end gaming GPUs.

## **Overview**

We believe in partnership over competition in terms of building a large game computing network. Therefore, we do not aim to profit from platform computing; we simply aim to match game demand with DePIN partners and gamers. This setup will benefit all stakeholders:    &#x20;

1. **Game Developers:** No setup is needed; just connect developers' wallets for cost settlement, and inferencing tasks are directed to the Network. The abundant supply keeps computes affordable.
2. **Players:** Our AI models are deployed in gaming GPUs for inferencing. Players can earn tokens by supplying computing resources with their high-end gaming laptops .
3. **DePIN Partners:** DePINs lack use cases because of GPU memory limitations, security concerns, and cost-effectiveness. Our Compute Network can provide DePIN partners with a true and sustainable use case. &#x20;

## Challenges

Three technical challenges that limit the use of decentralized inferencing for most AI use cases.

<table><thead><tr><th width="324">Challenges</th><th>Our Web3-Tailored Compute Use Case</th></tr></thead><tbody><tr><td><strong>GPU Memory Limitation:</strong> LLMs are too big to deploy on idled GPUs in DePINs and player laptops. </td><td><strong>Lightweight Model:</strong> <a href="breakthrough.md#model-quantization">quantized</a> and <a href="breakthrough.md#model-distillation">distilled</a> models to reduce hardware requirements of compute nodesâ€”RTX4090s sufficient for most features.  </td></tr><tr><td><strong>Security Concerns:</strong> hard to protect proprietary models nor verify inferencing in a trustless setup.</td><td><a href="breakthrough.md#model-obfuscation"><strong>Obfuscated</strong></a> <strong>Lightweight Model</strong>: models are "unreadable" for core technology protection. <a href="breakthrough.md#proof-of-inference">Proof of inferencing</a> is embedded into the models.    </td></tr><tr><td><strong>Not Cost-Effective:</strong> adding gas fee onto the inferencing cost makes decentralized compute expensive. </td><td><strong>Minimal Gas Fee:</strong> highly concentrated and long-term compute purchasers of platform games. <a href="architecture.md#id-1.-task-process">We post compute record per block but settle once a day</a>.    </td></tr></tbody></table>

We aim to bring a viable use case to decentralized inferencing networks by sharing our proprietary, obfuscated, lightweight AI models, and by offering a daily gas fee settlement model enabled by our concentrated compute purchasers of games.&#x20;

\
<br>
